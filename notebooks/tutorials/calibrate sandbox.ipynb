{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox calibration\n",
    "with this notebook you can create or change the calibration of a sandbox setup.\n",
    "Use the sliders in the widget to set the dimensions and vertical range of the sandbox and align the projected image of the kinect with the box.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some remarks on setting up the box\n",
    "\n",
    "Please consider the following guidelines to make your sandbox experience as pleasant as possible:\n",
    "- use a rectangular box, ideally with semitransparent (frosted) or opaque walls.\n",
    "- position the kinect and projector in a central position over the box. the closer they are together the better!\n",
    "- make sure the kinect is perfectly horizontal (=parallel to the floor of the box)\n",
    "- the higer the distance between the sandbox and the sensor/projector the better! the distance should be at leat 1-1.5 times the longest dimension of your box! Don't worry if the sandbox does not cover the entire field of view of the kinect, the resolution will still be more than good enough in the end!\n",
    "- adjust the keystone of the projector so that the egdes of the image are orthogonal. \n",
    "- adjust the position of the sandbox (or the projector/kinect) that the projected image is parallel to the box edges and covers the sandbox completely.\n",
    "- set the projection mode that the image is not mirrored or flipped when you look down at the sand from the front of the box.\n",
    "\n",
    "Now let us start with the calibration!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./../../../open_AR_Sandbox\\sandbox\\sandbox.py:8: UserWarning: Freenect is not installed. if you are using the Kinect Version 2 on a windows machine, use the KinectV2 class!\n",
      "  warn('Freenect is not installed. if you are using the Kinect Version 2 on a windows machine, use the KinectV2 class!')\n",
      "./../../../gempy-1.16\\gempy\\plotting\\visualization.py:31: UserWarning: Vtk package is not installed. No vtk visualization available.\n",
      "  warnings.warn('Vtk package is not installed. No vtk visualization available.')\n",
      "./../../../gempy-1.16\\gempy\\plotting\\visualization.py:36: UserWarning: Steno 3D package is not installed. No 3D online visualization available.\n",
      "  warnings.warn('Steno 3D package is not installed. No 3D online visualization available.')\n",
      "./../../../gempy-1.16\\gempy\\data_management.py:34: UserWarning: qgrid package is not installed. No interactive dataframes available.\n",
      "  warnings.warn('qgrid package is not installed. No interactive dataframes available.')\n",
      "./../../../gempy-1.16\\gempy\\posterior_analysis.py:24: UserWarning: pymc (v2) package is not installed. No support for stochastic simulation posterior analysis.\n",
      "  warnings.warn(\"pymc (v2) package is not installed. No support for stochastic simulation posterior analysis.\")\n",
      "./../../../gempy-1.16\\gempy\\posterior_analysis.py:28: UserWarning: pymc (v3) package is not installed. No support for stochastic simulation posterior analysis.\n",
      "  warnings.warn(\"pymc (v3) package is not installed. No support for stochastic simulation posterior analysis.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version of Kinect using (1 or 2):2\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "sys.path.append('./../../../open_AR_Sandbox/')\n",
    "sys.path.append('./../../../gempy-1.16/')\n",
    "import sandbox.sandbox as sb\n",
    "#New by Daniel\n",
    "from sb import KinectV2\n",
    "\n",
    "#import Sandbox as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup the sandbox\n",
    "initialize the components of the sandbox and opens an output stream in a new Chrome tab. Drag the tab to the projector screen and set to fullscreen (F11).\n",
    "If this step kills the kernel make sure there is not another notebook running with an active Kinect instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "body_joints_to_color_space() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-20c5f69f9ff9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkinect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKinectV2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\PycharmProjects\\open_AR_Sandbox\\sandbox\\sandbox.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, version_kinect, dummy, mirror)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_mapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\open_AR_Sandbox\\sandbox\\sandbox.py\u001b[0m in \u001b[0;36mget_mapped\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \"\"\"\n\u001b[1;32m--> 254\u001b[1;33m         \u001b[0mmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkinect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody_joints_to_color_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: body_joints_to_color_space() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "kinect = sb.KinectV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "print(kinect.get_color().shape,kinect.get_frame().shape)\n",
    "a=kinect.get_color()\n",
    "b=np.flipud(kinect.get_frame())\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(a)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('color.npy',a)\n",
    "np.save('depth.npy',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = sb.Projector(resolution = (1280,800))\n",
    "projector.refresh = 100           #Set projector refresh interval in milliseconds. 100ms = 10Hz \n",
    "projector.work_directory=\"./temp\"\n",
    "projector.start_stream()          #Open an new Tab with sandbox output.\n",
    "calibration = sb.Calibration(projector, associated_kinect=kinect)#projector.calibration\n",
    "projector.set_calibration(calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters:\n",
    "* *rot_angle:* rotation of kinect in respect to projector \n",
    "* *x_lim, y_lim:* horizontal and vertical extent of projeced image in pixels\n",
    "* *x_pos, y_pos:* offset of projected frame from top left corner \n",
    "* *scale_factor:* scaling factor from kinect pixels to projector pixels \n",
    "* *z_range:* vertical extend of the sandbox in mm distance from the kinect sensor. the vertical model extend is later mapped to these these values, so choose a z_range and model extend that is in the correct ratio with the horizontal dimension to avoid vertical exaggeration.\n",
    "* *box_width, box_height:* horizontal extent of th sandbox in millimeters. THese values are only used to calculate the scale of the model but have no further effect.\n",
    "* *legend:* switch legend on/off and set its size and position.\n",
    "* *profile:* an area to display additional information like a profile. Currently under development.  \n",
    "* *hot_area:* define an are to interact with the sandbox with markers (QR codes). Currently under development\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load calibration data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_infilename = \"sandbox_v2.dat\" \n",
    "calibration.load(calibration_file=calibration_infilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optional: adjust appearance of calibration frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration.contours = True     # show or hide contours\n",
    "calibration.n_contours = 20     # number of contours between z_min and z_max\n",
    "calibration.cmap = 'viridis' # name of colormap \n",
    "# calibration.contour_levels      # show the contour levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### open calibration widget \n",
    "### Parameters:\n",
    "* *rot_angle:* rotation of kinect in respect to projector \n",
    "* *x_lim, y_lim:* horizontal and vertical extent of projeced image in pixels\n",
    "* *x_pos, y_pos:* offset of projected frame from top left corner \n",
    "* *scale_factor:* scaling factor from kinect pixels to projector pixels \n",
    "* *z_range:* vertical extend of the sandbox in mm distance from the kinect sensor. the vertical model extend is later mapped to these these values, so choose a z_range and model extend that is in the correct ratio with the horizontal dimension to avoid vertical exaggeration.\n",
    "* *box_width, box_height:* horizontal extent of th sandbox in millimeters. THese values are only used to calculate the scale of the model but have no further effect.\n",
    "* *legend:* switch legend on/off and set its size and position.\n",
    "* *profile:* an area to display additional information like a profile. Currently under development.  \n",
    "* *hot_area:* define an are to interact with the sandbox with markers (QR codes). Currently under development\n",
    "\n",
    "### how to calibrate\n",
    "In the beginning you will see a depth image of the kinect in the upper left corner of the sandbox that is most probably not aligned at all with the sand. In the following we will step-by-step adjust the parameters to align the kinect, projector sandbox with each other. \n",
    "the image updates each time the value of a slider is changed. \n",
    "\n",
    "1. first of all it is time to get your hands dirty: make a huge pile of sand in the middle of the box. try to build it as high as possible or reasonable. and make sure that you have some spots on the left and right of the pile where is no sand left and you can see the bottom of the box. \n",
    "2. now adjust the z-range sliders to define the top and bottom of the box. pixels outside the range as well as invalid pixels that the kinect does not recognize atre white.  If possible you should set it up that the (z_max - z_min) is equal to the shorter horizontal dimension of the box.\n",
    "3. adjust rot_angle to make the edges of the box in the image parallel to the edges of the real box. Make sure the image is not upside down.\n",
    "4. adjust the left x_lim slider to cut away everything that is left of the box boundary. \n",
    "5. do the same for the top edge by adjusting the right y_lim slider.\n",
    "6. now align the top left corner of the image with the top left corner of the sandbox by adjusting x_pos and y_pos\n",
    "7. increase the scale_factor slider to make the projected image align with the real topography in the sandbox. use the bottom right corner of the sandbox as reference. \n",
    "8.adjust the right x_lim and the left y_lim sliders to cut off the image at he bottom and right edge. \n",
    "\n",
    "click \"close calibration\" when you are finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save calibration data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_outfilename = \"sandbox_v2.dat\" \n",
    "calibration.save(calibration_file=calibration_outfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projector.start_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
